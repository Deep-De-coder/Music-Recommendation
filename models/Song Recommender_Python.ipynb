{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a song recommender"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------\r\n",
    "Dataset used:\r\n",
    "-------------\r\n",
    "Million Songs Dataset\r\n",
    "Source: http://labrosa.ee.columbia.edu/millionsong/\r\n",
    "Paper: http://ismir2011.ismir.net/papers/OS6-1.pdf\r\n",
    "\r\n",
    "The current notebook uses a subset of the above data containing 10,000 songs obtained from:\r\n",
    "https://github.com/turi-code/tutorials/blob/master/notebooks/recsys_rank_10K_song.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d062b3c33d4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "\r\n",
    "import pandas\r\n",
    "from sklearn.cross_validation import train_test_split\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from sklearn.externals import joblib\r\n",
    "import Recommenders as Recommenders\r\n",
    "import Evaluation as Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load music data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Read userid-songid-listen_count triplets\r\n",
    "#This step might take time to download data from external sources\r\n",
    "triplets_file = 'https://static.turi.com/datasets/millionsong/10000.txt'\r\n",
    "songs_metadata_file = 'https://static.turi.com/datasets/millionsong/song_data.csv'\r\n",
    "\r\n",
    "song_df_1 = pandas.read_table(triplets_file,header=None)\r\n",
    "song_df_1.columns = ['user_id', 'song_id', 'listen_count']\r\n",
    "\r\n",
    "#Read song  metadata\r\n",
    "song_df_2 =  pandas.read_csv(songs_metadata_file)\r\n",
    "\r\n",
    "#Merge the two dataframes above to create input dataframe for recommender systems\r\n",
    "song_df = pandas.merge(song_df_1, song_df_2.drop_duplicates(['song_id']), on=\"song_id\", how=\"left\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data\n",
    "\n",
    "Music data shows how many times a user listened to a song, as well as the details of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "song_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(song_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "song_df = song_df.head(10000)\r\n",
    "\r\n",
    "#Merge song title and artist_name columns to make a merged column\r\n",
    "song_df['song'] = song_df['title'].map(str) + \" - \" + song_df['artist_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the most popular songs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "song_grouped = song_df.groupby(['song']).agg({'listen_count': 'count'}).reset_index()\r\n",
    "grouped_sum = song_grouped['listen_count'].sum()\r\n",
    "song_grouped['percentage']  = song_grouped['listen_count'].div(grouped_sum)*100\r\n",
    "song_grouped.sort_values(['listen_count', 'song'], ascending = [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of unique users in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "users = song_df['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1. Count the number of unique songs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "###Fill in the code here\r\n",
    "songs = song_df['song'].unique()\r\n",
    "len(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a song recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(song_df, test_size = 0.20, random_state=0)\r\n",
    "print(train_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple popularity-based recommender class (Can be used as a black box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Recommenders.popularity_recommender_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of popularity based recommender class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pm = Recommenders.popularity_recommender_py()\r\n",
    "pm.create(train_data, 'user_id', 'song')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the popularity model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "user_id = users[5]\r\n",
    "pm.recommend(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2: Use the popularity based model to make predictions for the following user id (Note the difference in recommendations from the first user id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "###Fill in the code here\r\n",
    "user_id = users[8]\r\n",
    "pm.recommend(user_id)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a song recommender with personalization\n",
    "\n",
    "We now create an item similarity based collaborative filtering model that allows us to make personalized recommendations to each user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for an item similarity based personalized recommender system (Can be used as a black box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Recommenders.item_similarity_recommender_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of item similarity based recommender class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "is_model = Recommenders.item_similarity_recommender_py()\r\n",
    "is_model.create(train_data, 'user_id', 'song')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the personalized model to make some song recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Print the songs for the user in training data\r\n",
    "user_id = users[5]\r\n",
    "user_items = is_model.get_user_items(user_id)\r\n",
    "#\r\n",
    "print(\"------------------------------------------------------------------------------------\")\r\n",
    "print(\"Training data songs for the user userid: %s:\" % user_id)\r\n",
    "print(\"------------------------------------------------------------------------------------\")\r\n",
    "\r\n",
    "for user_item in user_items:\r\n",
    "    print(user_item)\r\n",
    "\r\n",
    "print(\"----------------------------------------------------------------------\")\r\n",
    "print(\"Recommendation process going on:\")\r\n",
    "print(\"----------------------------------------------------------------------\")\r\n",
    "\r\n",
    "#Recommend songs for the user using personalized model\r\n",
    "is_model.recommend(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 3. Use the personalized model to make recommendations for the following user id. (Note the difference in recommendations from the first user id.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "user_id = users[7]\r\n",
    "#Fill in the code here\r\n",
    "user_items = is_model.get_user_items(user_id)\r\n",
    "#\r\n",
    "print(\"------------------------------------------------------------------------------------\")\r\n",
    "print(\"Training data songs for the user userid: %s:\" % user_id)\r\n",
    "print(\"------------------------------------------------------------------------------------\")\r\n",
    "\r\n",
    "for user_item in user_items:\r\n",
    "    print(user_item)\r\n",
    "\r\n",
    "print(\"----------------------------------------------------------------------\")\r\n",
    "print(\"Recommendation process going on:\")\r\n",
    "print(\"----------------------------------------------------------------------\")\r\n",
    "\r\n",
    "#Recommend songs for the user using personalized model\r\n",
    "is_model.recommend(user_id)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also apply the model to find similar songs to any song in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "is_model.get_similar_items(['U Smile - Justin Bieber'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 4. Use the personalized recommender model to get similar songs for the following song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "song = 'Yellow - Coldplay'\r\n",
    "###Fill in the code here\r\n",
    "is_model.get_similar_items([song])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative comparison between the models\n",
    "\n",
    "We now formally compare the popularity and the personalized models using precision-recall curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to calculate precision and recall (This can be used as a black box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Evaluation.precision_recall_calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the above precision recall calculator class to calculate the evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Define what percentage of users to use for precision recall calculation\n",
    "user_sample = 0.05\n",
    "\n",
    "#Instantiate the precision_recall_calculator class\n",
    "pr = Evaluation.precision_recall_calculator(test_data, train_data, pm, is_model)\n",
    "\n",
    "#Call method to calculate precision and recall values\n",
    "(pm_avg_precision_list, pm_avg_recall_list, ism_avg_precision_list, ism_avg_recall_list) = pr.calculate_measures(user_sample)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to plot precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "#Method to generate precision and recall curve\n",
    "def plot_precision_recall(m1_precision_list, m1_recall_list, m1_label, m2_precision_list, m2_recall_list, m2_label):\n",
    "    pl.clf()    \n",
    "    pl.plot(m1_recall_list, m1_precision_list, label=m1_label)\n",
    "    pl.plot(m2_recall_list, m2_precision_list, label=m2_label)\n",
    "    pl.xlabel('Recall')\n",
    "    pl.ylabel('Precision')\n",
    "    pl.ylim([0.0, 0.20])\n",
    "    pl.xlim([0.0, 0.20])\n",
    "    pl.title('Precision-Recall curve')\n",
    "    #pl.legend(loc=\"upper right\")\n",
    "    pl.legend(loc=9, bbox_to_anchor=(0.5, -0.2))\n",
    "    pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Plotting precision recall curves.\")\n",
    "\n",
    "plot_precision_recall(pm_avg_precision_list, pm_avg_recall_list, \"popularity_model\",\n",
    "                      ism_avg_precision_list, ism_avg_recall_list, \"item_similarity_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Precision Recall curve using pickled results on a larger data subset(Python 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Plotting precision recall curves for a larger subset of data (100,000 rows) (user sample = 0.005).\")\n",
    "\n",
    "#Read the persisted files \n",
    "pm_avg_precision_list = joblib.load('pm_avg_precision_list_3.pkl')\n",
    "pm_avg_recall_list = joblib.load('pm_avg_recall_list_3.pkl')\n",
    "ism_avg_precision_list = joblib.load('ism_avg_precision_list_3.pkl')\n",
    "ism_avg_recall_list = joblib.load('ism_avg_recall_list_3.pkl')\n",
    "\n",
    "print(\"Plotting precision recall curves.\")\n",
    "plot_precision_recall(pm_avg_precision_list, pm_avg_recall_list, \"popularity_model\",\n",
    "                      ism_avg_precision_list, ism_avg_recall_list, \"item_similarity_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Precision Recall curve using pickled results on a larger data subset(Python 2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Plotting precision recall curves for a larger subset of data (100,000 rows) (user sample = 0.005).\")\n",
    "\n",
    "pm_avg_precision_list = joblib.load('pm_avg_precision_list_2.pkl')\n",
    "pm_avg_recall_list = joblib.load('pm_avg_recall_list_2.pkl')\n",
    "ism_avg_precision_list = joblib.load('ism_avg_precision_list_2.pkl')\n",
    "ism_avg_recall_list = joblib.load('ism_avg_recall_list_2.pkl')\n",
    "\n",
    "print(\"Plotting precision recall curves.\")\n",
    "plot_precision_recall(pm_avg_precision_list, pm_avg_recall_list, \"popularity_model\",\n",
    "                      ism_avg_precision_list, ism_avg_recall_list, \"item_similarity_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve shows that the personalized model provides much better performance over the popularity model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization based Recommender System"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using SVD matrix factorization based collaborative filtering recommender system\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "The following code implements a Singular Value Decomposition (SVD) based matrix factorization collaborative filtering recommender system. The user ratings matrix used is a small matrix as follows:\n",
    "\n",
    "        Item0   Item1   Item2   Item3\n",
    "User0     3        1       2      3\n",
    "User1     4        3       4      3\n",
    "User2     3        2       1      5\n",
    "User3     1        6       5      2\n",
    "User4     0        0       5      0\n",
    "\n",
    "As we can see in the above matrix, all users except user 4 rate all items. The code calculates predicted recommendations for user 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code source written with help from: \n",
    "#http://antoinevastel.github.io/machine%20learning/python/2016/02/14/svd-recommender-system.html\n",
    "\n",
    "import math as mt\n",
    "import csv\n",
    "from sparsesvd import sparsesvd #used for matrix factorization\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix #used for sparse matrix\n",
    "from scipy.sparse.linalg import * #used for matrix multiplication\n",
    "\n",
    "#Note: You may need to install the library sparsesvd. Documentation for \n",
    "#sparsesvd method can be found here:\n",
    "#https://pypi.python.org/pypi/sparsesvd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to compute SVD and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants defining the dimensions of our User Rating Matrix (URM)\n",
    "MAX_PID = 4\n",
    "MAX_UID = 5\n",
    "\n",
    "#Compute SVD of the user ratings matrix\n",
    "def computeSVD(urm, K):\n",
    "    U, s, Vt = sparsesvd(urm, K)\n",
    "\n",
    "    dim = (len(s), len(s))\n",
    "    S = np.zeros(dim, dtype=np.float32)\n",
    "    for i in range(0, len(s)):\n",
    "        S[i,i] = mt.sqrt(s[i])\n",
    "\n",
    "    U = csc_matrix(np.transpose(U), dtype=np.float32)\n",
    "    S = csc_matrix(S, dtype=np.float32)\n",
    "    Vt = csc_matrix(Vt, dtype=np.float32)\n",
    "    \n",
    "    return U, S, Vt\n",
    "\n",
    "#Compute estimated rating for the test user\n",
    "def computeEstimatedRatings(urm, U, S, Vt, uTest, K, test):\n",
    "    rightTerm = S*Vt \n",
    "\n",
    "    estimatedRatings = np.zeros(shape=(MAX_UID, MAX_PID), dtype=np.float16)\n",
    "    for userTest in uTest:\n",
    "        prod = U[userTest, :]*rightTerm\n",
    "        #we convert the vector to dense format in order to get the indices \n",
    "        #of the movies with the best estimated ratings \n",
    "        estimatedRatings[userTest, :] = prod.todense()\n",
    "        recom = (-estimatedRatings[userTest, :]).argsort()[:250]\n",
    "    return recom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SVD to make predictions for a test user id, say 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Used in SVD calculation (number of latent factors)\n",
    "K=2\n",
    "\n",
    "#Initialize a sample user rating matrix\n",
    "urm = np.array([[3, 1, 2, 3],[4, 3, 4, 3],[3, 2, 1, 5], [1, 6, 5, 2], [5, 0,0 , 0]])\n",
    "urm = csc_matrix(urm, dtype=np.float32)\n",
    "\n",
    "#Compute SVD of the input user ratings matrix\n",
    "U, S, Vt = computeSVD(urm, K)\n",
    "\n",
    "#Test user set as user_id 4 with ratings [0, 0, 5, 0]\n",
    "uTest = [4]\n",
    "print(\"User id for whom recommendations are needed: %d\" % uTest[0])\n",
    "\n",
    "#Get estimated rating for test user\n",
    "print(\"Predictied ratings:\")\n",
    "uTest_recommended_items = computeEstimatedRatings(urm, U, S, Vt, uTest, K, True)\n",
    "print(uTest_recommended_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a.) Change the input matrix row for test userid 4 in the user ratings matrix to the following value. Note the difference in predicted recommendations in this case.\n",
    "\n",
    "i.) [5 0 0 0]\n",
    "\n",
    "\n",
    "(Note*: The predicted ratings by the code include the items already rated by test user as well. This has been left purposefully like this for better understanding of SVD).\n",
    "\n",
    "SVD tutorial: http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Intuition behind SVD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVD result gives three matrices as output: U, S and Vt (T in Vt means transpose). Matrix U represents user vectors and Matrix Vt represents item vectors. In simple terms, U represents users as 2 dimensional points in the latent vector space, and Vt represents items as 2 dimensional points in the same space.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we print the matrices U, S and Vt and try to interpret them. Think how the points for users and items will look like in a 2 dimensional axis. For example, the following code plots all user vectors from the matrix U in the 2 dimensional space. Similarly, we plot all the item vectors in the same plot from the matrix Vt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "\n",
    "#Plot all the users\n",
    "print(\"Matrix Dimensions for U\")\n",
    "print(U.shape)\n",
    "\n",
    "for i in range(0, U.shape[0]):\n",
    "    plot(U[i,0], U[i,1], marker = \"*\", label=\"user\"+str(i))\n",
    "\n",
    "for j in range(0, Vt.T.shape[0]):\n",
    "    plot(Vt.T[j,0], Vt.T[j,1], marker = 'd', label=\"item\"+str(j))    \n",
    "    \n",
    "legend(loc=\"upper right\")\n",
    "title('User vectors in the Latent semantic space')\n",
    "ylim([-0.7, 0.7])\n",
    "xlim([-0.7, 0])\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cad7999a67a6ba8e4a147ba388f9dc226033e86866ce6ebabbc7d4486bf9a0bb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
